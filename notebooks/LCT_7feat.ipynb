{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5be21e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:24.536937Z",
     "start_time": "2024-02-07T00:15:22.181856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /opt/conda/lib/python3.10/site-packages (0.61.0)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from memory_profiler) (5.9.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863be33e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.034792Z",
     "start_time": "2024-02-07T00:15:24.538692Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "import argparse\n",
    "sys.path.append('../')\n",
    "\n",
    "# load torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load custom modules required for jetCLR training\n",
    "from scripts.modules.jet_augs import rotate_jets, distort_jets, rescale_pts, crop_jets, translate_jets, collinear_fill_jets\n",
    "# from scripts.modules.transformer import Transformer\n",
    "from scripts.modules.losses import contrastive_loss, align_loss, uniform_loss\n",
    "from scripts.modules.perf_eval import get_perf_stats, linear_classifier_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3f0b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.038897Z",
     "start_time": "2024-02-07T00:15:26.036232Z"
    }
   },
   "outputs": [],
   "source": [
    "# import standard python modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# import torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import simple FCN network\n",
    "from scripts.modules.fcn_linear import fully_connected_linear_network\n",
    "from scripts.modules.fcn import fully_connected_network\n",
    "\n",
    "# import preprocessing functions\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac342f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.056190Z",
     "start_time": "2024-02-07T00:15:26.040112Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class for transformer network\n",
    "class Transformer(nn.Module):\n",
    "    # define and intialize the structure of the neural network\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        model_dim,\n",
    "        output_dim,\n",
    "        n_heads,\n",
    "        dim_feedforward,\n",
    "        n_layers,\n",
    "        learning_rate,\n",
    "        n_head_layers=2,\n",
    "        head_norm=False,\n",
    "        dropout=0.1,\n",
    "        opt=\"adam\",\n",
    "        log=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # define hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_head_layers = n_head_layers\n",
    "        self.head_norm = head_norm\n",
    "        self.dropout = dropout\n",
    "        self.log = log\n",
    "        # define subnetworks\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                model_dim, n_heads, dim_feedforward=dim_feedforward, dropout=dropout\n",
    "            ),\n",
    "            n_layers,\n",
    "        )\n",
    "        # head_layers have output_dim\n",
    "        if n_head_layers == 0:\n",
    "            self.head_layers = []\n",
    "        else:\n",
    "            if head_norm:\n",
    "                self.norm_layers = nn.ModuleList([nn.LayerNorm(model_dim)])\n",
    "            self.head_layers = nn.ModuleList([nn.Linear(model_dim, output_dim)])\n",
    "            for i in range(n_head_layers - 1):\n",
    "                if head_norm:\n",
    "                    self.norm_layers.append(nn.LayerNorm(output_dim))\n",
    "                self.head_layers.append(nn.Linear(output_dim, output_dim))\n",
    "        # option to use adam or sgd\n",
    "        if opt == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        if opt == \"sgdca\" or opt == \"sgdslr\" or opt == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(\n",
    "                self.parameters(), lr=self.learning_rate, momentum=0.9\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inpt,\n",
    "        mask=None,\n",
    "        use_mask=False,\n",
    "        use_continuous_mask=False,\n",
    "        mult_reps=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input here is (batch_size, n_constit, 3 or 7)\n",
    "        but transformer expects (n_constit, batch_size, 3 or 7) so we need to transpose\n",
    "        if use_mask is True, will mask out all inputs with pT=0\n",
    "        \"\"\"\n",
    "#         print(f\"input shape: {inpt.shape}\")\n",
    "        assert not (use_mask and use_continuous_mask)\n",
    "        pt_index = 2 if args.full_kinematics else 0\n",
    "        # make a copy\n",
    "        x = inpt + 0.0\n",
    "        if use_mask:\n",
    "            pT_zero = x[:, :, pt_index] == 0\n",
    "        if use_continuous_mask:\n",
    "            if self.log:\n",
    "                log_pT = x[:, :, pt_index]\n",
    "                # exponentiate to get actual pT\n",
    "                pT = torch.where(log_pT != 0, torch.exp(log_pT), torch.zeros_like(log_pT))\n",
    "            else:\n",
    "                pT = x[:, :, pt_index]\n",
    "#             print(f\"pT: {pT}\")\n",
    "        if use_mask:\n",
    "            mask = self.make_mask(pT_zero).to(x.device)\n",
    "        elif use_continuous_mask:\n",
    "            mask = self.make_continuous_mask(pT).to(x.device)\n",
    "        else:\n",
    "            mask = None\n",
    "#         print(f\"mask : {mask}\")\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        # (n_constit, batch_size, model_dim)\n",
    "        x = self.embedding(x)\n",
    "#         print(f\"after embedding: {x.shape}\")\n",
    "        x = self.transformer(x, mask=mask)\n",
    "#         print(f\"after transformer: {x.shape}\")\n",
    "        if use_mask:\n",
    "            # set masked constituents to zero\n",
    "            # otherwise the sum will change if the constituents with 0 pT change\n",
    "            x[torch.transpose(pT_zero, 0, 1)] = 0\n",
    "        elif use_continuous_mask:\n",
    "            # scale x by pT, so that function is IR safe\n",
    "            # transpose first to get correct shape\n",
    "#             x *= torch.transpose(pT, 0, 1)[:, :, None]\n",
    "            pass\n",
    "        # sum over sequence dim\n",
    "        # (batch_size, model_dim)\n",
    "        x = x.sum(0)\n",
    "#         print(f\"after summing: {x.shape}\")\n",
    "        return self.head(x, mult_reps)\n",
    "\n",
    "    def head(self, x, mult_reps):\n",
    "        \"\"\"\n",
    "        calculates output of the head if it exists, i.e. if n_head_layer>0\n",
    "        returns multiple representation layers if asked for by mult_reps = True\n",
    "        input:  x shape=(batchsize, model_dim)\n",
    "                mult_reps boolean\n",
    "        output: reps shape=(batchsize, output_dim)                  for mult_reps=False\n",
    "                reps shape=(batchsize, number_of_reps, output_dim)  for mult_reps=True\n",
    "        \"\"\"\n",
    "        relu = nn.ReLU()\n",
    "        if mult_reps == True:\n",
    "            if self.n_head_layers > 0:\n",
    "                reps = torch.empty(x.shape[0], self.n_head_layers + 1, self.output_dim)\n",
    "                # Transform x to output_dim size before assignment\n",
    "                x_transformed = (\n",
    "                    self.head_layers[0](relu(x)) if self.n_head_layers > 0 else x\n",
    "                )\n",
    "                reps[:, 0] = x_transformed\n",
    "                for i, layer in enumerate(self.head_layers):\n",
    "                    if self.head_norm:\n",
    "                        x = self.norm_layers[i](x)\n",
    "                    x = relu(x)\n",
    "                    x = layer(x)\n",
    "                    reps[:, i + 1] = x\n",
    "                return reps\n",
    "            else:\n",
    "                reps = x[:, None, :]\n",
    "                return reps\n",
    "        else:\n",
    "            for i, layer in enumerate(self.head_layers):\n",
    "                if self.head_norm:\n",
    "                    x = self.norm_layers[i](x)\n",
    "                x = relu(x)\n",
    "                x = layer(x)\n",
    "            return x\n",
    "\n",
    "    def forward_batchwise(\n",
    "        self, x, batch_size, use_mask=False, use_continuous_mask=False\n",
    "    ):\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            if self.n_head_layers == 0:\n",
    "                rep_dim = self.model_dim\n",
    "                number_of_reps = 1\n",
    "            elif self.n_head_layers > 0:\n",
    "                rep_dim = self.output_dim\n",
    "                number_of_reps = self.n_head_layers + 1\n",
    "            out = torch.empty(x.size(0), number_of_reps, rep_dim)\n",
    "            idx_list = torch.split(torch.arange(x.size(0)), batch_size)\n",
    "            for idx in idx_list:\n",
    "                output = (\n",
    "                    self(\n",
    "                        x[idx].to(device),\n",
    "                        use_mask=use_mask,\n",
    "                        use_continuous_mask=use_continuous_mask,\n",
    "                        mult_reps=True,\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                )\n",
    "                out[idx] = output\n",
    "        return out\n",
    "\n",
    "    def make_mask(self, pT_zero):\n",
    "        \"\"\"\n",
    "        Input: batch of bools of whether pT=0, shape (batchsize, n_constit)\n",
    "        Output: mask for transformer model which masks out constituents with pT=0, shape (batchsize*n_transformer_heads, n_constit, n_constit)\n",
    "        mask is added to attention output before softmax: 0 means value is unchanged, -inf means it will be masked\n",
    "        \"\"\"\n",
    "        n_constit = pT_zero.size(1)\n",
    "        pT_zero = torch.repeat_interleave(pT_zero, self.n_heads, axis=0)\n",
    "        pT_zero = torch.repeat_interleave(pT_zero[:, None], n_constit, axis=1)\n",
    "        mask = torch.zeros(pT_zero.size(0), n_constit, n_constit)\n",
    "        mask[pT_zero] = -np.inf\n",
    "#         print(f\"mask: {mask}\")\n",
    "        return mask\n",
    "\n",
    "    def make_continuous_mask(self, pT):\n",
    "        \"\"\"\n",
    "        Input: batch of pT values, shape (batchsize, n_constit)\n",
    "        Output: mask for transformer model: -1/pT, shape (batchsize*n_transformer_heads, n_constit, n_constit)\n",
    "        mask is added to attention output before softmax: 0 means value is unchanged, -inf means it will be masked\n",
    "        intermediate values mean it is partly masked\n",
    "        This function implements IR safety in the transformer\n",
    "        \"\"\"\n",
    "#         print(f\"pT : {pT}\")\n",
    "        n_constit = pT.size(1)\n",
    "        pT_reshape = torch.repeat_interleave(pT, self.n_heads, axis=0)\n",
    "        pT_reshape = torch.repeat_interleave(pT_reshape[:, None], n_constit, axis=1)\n",
    "        # mask = -1/pT_reshape\n",
    "        mask = 0.5 * torch.log(pT_reshape)\n",
    "#         print(f\"mask: {mask}\")\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c9bcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.061876Z",
     "start_time": "2024-02-07T00:15:26.058024Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_path, flag, n_files=-1):\n",
    "    if args.full_kinematics:\n",
    "        data_files = glob.glob(f\"{dataset_path}/{flag}/processed/7_features_raw/data/*\")\n",
    "    else:\n",
    "        data_files = glob.glob(f\"{dataset_path}/{flag}/processed/3_features_raw/data/*\")\n",
    "\n",
    "    data = []\n",
    "    for i, file in enumerate(data_files):\n",
    "        if args.full_kinematics:\n",
    "            data.append(np.load(f\"{dataset_path}/{flag}/processed/7_features_raw/data/data_{i}.npy\")) \n",
    "        else:\n",
    "            data.append(np.load(f\"{dataset_path}/{flag}/processed/3_features_raw/data/data_{i}.pt\")) \n",
    "        print(f\"--- loaded file {i} from `{flag}` directory\")\n",
    "        if n_files != -1 and i == n_files - 1:\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_labels(dataset_path, flag, n_files=-1):\n",
    "    data_files = glob.glob(f\"{dataset_path}/{flag}/processed/7_features_raw/labels/*\")\n",
    "\n",
    "    data = []\n",
    "    for i, file in enumerate(data_files):\n",
    "        data.append(np.load(f\"{dataset_path}/{flag}/processed/7_features_raw/labels/labels_{i}.npy\"))\n",
    "        print(f\"--- loaded label file {i} from `{flag}` directory\")\n",
    "        if n_files != -1 and i == n_files - 1:\n",
    "            break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35304f15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.065135Z",
     "start_time": "2024-02-07T00:15:26.063088Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971b285d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.069085Z",
     "start_time": "2024-02-07T00:15:26.066292Z"
    }
   },
   "outputs": [],
   "source": [
    "inpput_dim = 7\n",
    "args.sbratio = 1\n",
    "args.output_dim = 1000\n",
    "args.model_dim = 1000 \n",
    "args.n_heads = 4\n",
    "args.dim_feedforward= 1000\n",
    "args.n_layers= 4 \n",
    "args.learning_rate = 0.00005 \n",
    "args.n_head_layers = 2 \n",
    "args.opt = \"adam\"\n",
    "args.label = \"test-aug-7-300\"\n",
    "args.load_path = f\"/ssl-jet-vol-v2/JetCLR/models/experiments/{args.label}/final_model.pt\"\n",
    "args.trs = True\n",
    "args.mask = False\n",
    "args.cmask = True\n",
    "args.batch_size = 128\n",
    "args.trsw = 0.1\n",
    "args.full_kinematics = True\n",
    "args.num_files = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bacb4bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.196374Z",
     "start_time": "2024-02-07T00:15:26.070240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "--- loaded file 0 from `train` directory\n",
      "--- loaded label file 0 from `train` directory\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print( \"loading data\")\n",
    "data = load_data(\"/ssl-jet-vol-v2/toptagging\", \"train\", args.num_files)\n",
    "labels = load_labels(\"/ssl-jet-vol-v2/toptagging\", \"train\", args.num_files)\n",
    "tr_dat_in = np.concatenate(data, axis=0)  # Concatenate along the first axis\n",
    "tr_lab_in = np.concatenate(labels, axis=0)\n",
    "tr_dat_in = tr_dat_in[:10000]\n",
    "tr_lab_in = tr_lab_in[:10000]\n",
    "print(tr_lab_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a56cc1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:26.255761Z",
     "start_time": "2024-02-07T00:15:26.197752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  7\n",
      "shuffling data and doing the S/B split\n"
     ]
    }
   ],
   "source": [
    "# input dim to the transformer -> (pt,eta,phi)\n",
    "input_dim = tr_dat_in.shape[1]\n",
    "print(\"input_dim: \", input_dim)\n",
    "\n",
    "# creating the training dataset\n",
    "print( \"shuffling data and doing the S/B split\", flush=True )\n",
    "tr_bkg_dat = tr_dat_in[ tr_lab_in==0 ].copy()\n",
    "tr_sig_dat = tr_dat_in[ tr_lab_in==1 ].copy()\n",
    "nbkg_tr = int( tr_bkg_dat.shape[0] )\n",
    "nsig_tr = int( args.sbratio * nbkg_tr )\n",
    "list_tr_dat = list( tr_bkg_dat[ 0:nbkg_tr ] ) + list( tr_sig_dat[ 0:nsig_tr ] )\n",
    "list_tr_lab = [ 0 for i in range( nbkg_tr ) ] + [ 1 for i in range( nsig_tr ) ]\n",
    "ldz_tr = list( zip( list_tr_dat, list_tr_lab ) )\n",
    "random.shuffle( ldz_tr )\n",
    "tr_dat, tr_lab = zip( *ldz_tr )\n",
    "# reducing the training data\n",
    "tr_dat = np.array( tr_dat )\n",
    "tr_lab = np.array( tr_lab )\n",
    "\n",
    "# create two validation sets: \n",
    "# one for training the linear classifier test (LCT)\n",
    "# and one for testing on it\n",
    "# we will do this just with tr_dat_in, but shuffled and split 50/50\n",
    "# this should be fine because the jetCLR training doesn't use labels\n",
    "# we want the LCT to use S/B=1 all the time\n",
    "list_vl_dat = list( tr_dat_in.copy() )\n",
    "list_vl_lab = list( tr_lab_in.copy() )\n",
    "ldz_vl = list( zip( list_vl_dat, list_vl_lab ) )\n",
    "random.shuffle( ldz_vl )\n",
    "vl_dat, vl_lab = zip( *ldz_vl )\n",
    "vl_dat = np.array( vl_dat )\n",
    "vl_lab = np.array( vl_lab )\n",
    "vl_len = vl_dat.shape[0]\n",
    "vl_split_len = int( vl_len/2 )\n",
    "vl_dat_1 = vl_dat[ 0:vl_split_len ]\n",
    "vl_lab_1 = vl_lab[ 0:vl_split_len ]\n",
    "vl_dat_2 = vl_dat[ -vl_split_len: ]\n",
    "vl_lab_2 = vl_lab[ -vl_split_len: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39398fef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:27.981625Z",
     "start_time": "2024-02-07T00:15:26.257219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising the network\n",
      "peak memory: 1126.80 MiB, increment: 107.85 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set-up parameters for the LCT\n",
    "linear_input_size = args.output_dim\n",
    "linear_n_epochs = 750\n",
    "linear_learning_rate = 0.001\n",
    "linear_batch_size = 128\n",
    "\n",
    "# initialise the network\n",
    "print( \"initialising the network\", flush=True )\n",
    "%memit net = Transformer( input_dim, args.model_dim, args.output_dim, args.n_heads, args.dim_feedforward, args.n_layers, args.learning_rate, args.n_head_layers, dropout=0.1, opt=args.opt, log=args.full_kinematics )\n",
    "# send network to device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "net.to( device )\n",
    "# print(net)\n",
    "net.load_state_dict(torch.load(f\"{args.load_path}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2b1264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:27.986247Z",
     "start_time": "2024-02-07T00:15:27.983731Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# def bytes_to_gb(size_in_bytes):\n",
    "#     \"\"\"Convert bytes to gigabytes.\"\"\"\n",
    "#     return size_in_bytes / (1024**3)\n",
    "\n",
    "# global_items = list(globals().items())  # Create a list of global items\n",
    "# threshold_mb = 10  # Threshold in MB\n",
    "# threshold_bytes = threshold_mb * (1024**2)  # Convert threshold to bytes\n",
    "\n",
    "# print(f\"{'Variable Name':<20} {'Size (GB)':>10}\")\n",
    "# print('-' * 32)\n",
    "\n",
    "# for var_name, value in global_items:\n",
    "#     size_in_bytes = sys.getsizeof(value)\n",
    "#     if size_in_bytes > threshold_bytes:\n",
    "#         size_in_gb = bytes_to_gb(size_in_bytes)\n",
    "#         print(f\"{var_name:<20} {size_in_gb:>10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabdbf7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:36.652611Z",
     "start_time": "2024-02-07T00:15:27.987579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the final LCT run\n",
      "obtaining representations\n",
      "finished obtaining representations, starting LCT\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "print( \"starting the final LCT run\", flush=True )\n",
    "print(\"obtaining representations\")\n",
    "# evaluate the network on the testing data, applying some augmentations first if it's required\n",
    "# if args.trs:\n",
    "#     vl_dat_1 = translate_jets( vl_dat_1, width=args.trsw )\n",
    "#     vl_dat_2 = translate_jets( vl_dat_2, width=args.trsw )\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    #vl_reps_1 = F.normalize( net.forward_batchwise( torch.Tensor( vl_dat_1 ).transpose(1,2), args.batch_size, use_mask=args.mask, use_continuous_mask=args.cmask ).detach().cpu(), dim=-1 ).numpy()\n",
    "    #vl_reps_2 = F.normalize( net.forward_batchwise( torch.Tensor( vl_dat_2 ).transpose(1,2), args.batch_size, use_mask=args.mask, use_continuous_mask=args.cmask ).detach().cpu(), dim=-1 ).numpy()\n",
    "    vl_reps_1 = net.forward_batchwise( torch.Tensor( vl_dat_1 ).transpose(1,2), args.batch_size, use_mask=args.mask, use_continuous_mask=args.cmask ).detach().cpu().numpy()\n",
    "    vl_reps_2 = net.forward_batchwise( torch.Tensor( vl_dat_2 ).transpose(1,2), args.batch_size, use_mask=args.mask, use_continuous_mask=args.cmask ).detach().cpu().numpy()\n",
    "    net.train()\n",
    "#     del vl_dat_1, vl_dat_2\n",
    "#     del net\n",
    "#     gc.collect()\n",
    "print(\"finished obtaining representations, starting LCT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ffbac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:15:36.924946Z",
     "start_time": "2024-02-07T00:15:36.654680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tr_dat_in, tr_bkg_dat, tr_sig_dat, tr_dat, vl_dat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c67870df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:19:05.047021Z",
     "start_time": "2024-02-07T00:15:36.927680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA A10\n",
      "49.86111\n",
      "re-initialized LCT\n",
      "8.639498\n",
      "(rep layer 0) epoch: 0, loss: 29.526554, val loss: 15.651224\n",
      "(rep layer 0) epoch: 25, loss: 8.260195, val loss: 14.25308\n",
      "(rep layer 0) epoch: 50, loss: 8.639498, val loss: 10.515268\n",
      "(rep layer 0) epoch: 75, loss: 11.006305, val loss: 9.1581745\n",
      "(rep layer 0) epoch: 100, loss: 4.239189, val loss: 7.8309126\n",
      "(rep layer 0) epoch: 125, loss: 4.095844, val loss: 7.1283646\n",
      "(rep layer 0) epoch: 150, loss: 2.9051363, val loss: 5.998016\n",
      "(rep layer 0) epoch: 175, loss: 3.8697696, val loss: 6.7341166\n",
      "(rep layer 0) epoch: 200, loss: 5.6250305, val loss: 6.497891\n",
      "(rep layer 0) epoch: 225, loss: 2.9808502, val loss: 5.307939\n",
      "(rep layer 0) epoch: 250, loss: 2.3099823, val loss: 6.3966956\n",
      "(rep layer 0) epoch: 275, loss: 2.1831696, val loss: 8.391468\n",
      "(rep layer 0) epoch: 300, loss: 1.563565, val loss: 7.41227\n",
      "(rep layer 0) epoch: 325, loss: 1.8401449, val loss: 6.4278436\n",
      "(rep layer 0) epoch: 350, loss: 1.7700554, val loss: 6.4312167\n",
      "(rep layer 0) epoch: 375, loss: 1.0111698, val loss: 11.983672\n",
      "(rep layer 0) epoch: 400, loss: 1.8934457, val loss: 5.2728233\n",
      "(rep layer 0) epoch: 425, loss: 1.608993, val loss: 5.7481036\n",
      "(rep layer 0) epoch: 450, loss: 0.8660342, val loss: 4.520575\n",
      "(rep layer 0) epoch: 475, loss: 1.1403412, val loss: 5.7781625\n",
      "(rep layer 0) epoch: 500, loss: 1.030267, val loss: 7.0012913\n",
      "(rep layer 0) epoch: 525, loss: 0.9212973, val loss: 9.894963\n",
      "(rep layer 0) epoch: 550, loss: 1.6360182, val loss: 3.990525\n",
      "(rep layer 0) epoch: 575, loss: 1.0865984, val loss: 11.146119\n",
      "(rep layer 0) epoch: 600, loss: 0.8631667, val loss: 5.9031873\n",
      "(rep layer 0) epoch: 625, loss: 0.90648335, val loss: 5.811687\n",
      "(rep layer 0) epoch: 650, loss: 1.564558, val loss: 4.5991015\n",
      "(rep layer 0) epoch: 675, loss: 3.6244931, val loss: 6.4451632\n",
      "(rep layer 0) epoch: 700, loss: 3.704255, val loss: 3.1408498\n",
      "(rep layer 0) epoch: 725, loss: 1.2472128, val loss: 7.595068\n",
      "run 0 (rep layer 0) auc: 0.9329\n",
      "run 0 (rep layer 0) imtafe: 21.9\n",
      "Device 0: NVIDIA A10\n",
      "49.879807\n",
      "re-initialized LCT\n",
      "49.648438\n",
      "re-initialized LCT\n",
      "1.6199682\n",
      "(rep layer 1) epoch: 0, loss: 14.94438, val loss: 11.160649\n",
      "(rep layer 1) epoch: 25, loss: 3.634594, val loss: 5.386551\n",
      "(rep layer 1) epoch: 50, loss: 1.6199682, val loss: 4.3872414\n",
      "(rep layer 1) epoch: 75, loss: 1.0950168, val loss: 3.126371\n",
      "(rep layer 1) epoch: 100, loss: 2.100045, val loss: 2.3284338\n",
      "(rep layer 1) epoch: 125, loss: 0.84877956, val loss: 4.575168\n",
      "(rep layer 1) epoch: 150, loss: 0.8836749, val loss: 7.6300325\n",
      "(rep layer 1) epoch: 175, loss: 0.8483124, val loss: 11.345646\n",
      "(rep layer 1) epoch: 200, loss: 0.7307228, val loss: 5.4551673\n",
      "(rep layer 1) epoch: 225, loss: 1.1867732, val loss: 13.126347\n",
      "(rep layer 1) epoch: 250, loss: 1.682953, val loss: 6.817976\n",
      "(rep layer 1) epoch: 275, loss: 1.0268071, val loss: 4.4228706\n",
      "(rep layer 1) epoch: 300, loss: 0.552274, val loss: 6.091871\n",
      "(rep layer 1) epoch: 325, loss: 0.71004456, val loss: 2.2038531\n",
      "(rep layer 1) epoch: 350, loss: 0.6865437, val loss: 2.0834613\n",
      "(rep layer 1) epoch: 375, loss: 1.7255545, val loss: 6.8769584\n",
      "(rep layer 1) epoch: 400, loss: 1.2277255, val loss: 1.4269629\n",
      "(rep layer 1) epoch: 425, loss: 1.0923529, val loss: 1.8605183\n",
      "(rep layer 1) epoch: 450, loss: 0.78542066, val loss: 2.7481563\n",
      "(rep layer 1) epoch: 475, loss: 1.3025444, val loss: 1.2729923\n",
      "(rep layer 1) epoch: 500, loss: 0.6595918, val loss: 1.7741399\n",
      "(rep layer 1) epoch: 525, loss: 1.471194, val loss: 2.1672144\n",
      "(rep layer 1) epoch: 550, loss: 0.88847417, val loss: 4.738645\n",
      "(rep layer 1) epoch: 575, loss: 0.7805424, val loss: 1.6959382\n",
      "(rep layer 1) epoch: 600, loss: 0.46682814, val loss: 2.5644977\n",
      "(rep layer 1) epoch: 625, loss: 2.0451803, val loss: 1.7495294\n",
      "(rep layer 1) epoch: 650, loss: 0.58763564, val loss: 1.2568645\n",
      "(rep layer 1) epoch: 675, loss: 1.0375212, val loss: 1.4598873\n",
      "(rep layer 1) epoch: 700, loss: 1.2351446, val loss: 3.5671344\n",
      "(rep layer 1) epoch: 725, loss: 0.82633895, val loss: 2.1228237\n",
      "run 0 (rep layer 1) auc: 0.9323\n",
      "run 0 (rep layer 1) imtafe: 19.1\n",
      "Device 0: NVIDIA A10\n",
      "0.28367054\n",
      "(rep layer 2) epoch: 0, loss: 0.47410372, val loss: 0.33590066\n",
      "(rep layer 2) epoch: 25, loss: 0.30068997, val loss: 0.32775235\n",
      "(rep layer 2) epoch: 50, loss: 0.28367054, val loss: 0.32633203\n",
      "(rep layer 2) epoch: 75, loss: 0.27998498, val loss: 0.3164911\n",
      "(rep layer 2) epoch: 100, loss: 0.28641468, val loss: 0.31486642\n",
      "(rep layer 2) epoch: 125, loss: 0.2798473, val loss: 0.31027257\n",
      "(rep layer 2) epoch: 150, loss: 0.28257817, val loss: 0.3329995\n",
      "(rep layer 2) epoch: 175, loss: 0.2717291, val loss: 0.32586366\n",
      "(rep layer 2) epoch: 200, loss: 0.27216744, val loss: 0.31002426\n",
      "(rep layer 2) epoch: 225, loss: 0.27182, val loss: 0.32893482\n",
      "(rep layer 2) epoch: 250, loss: 0.27446118, val loss: 0.3165348\n",
      "(rep layer 2) epoch: 275, loss: 0.27494448, val loss: 0.32271925\n",
      "(rep layer 2) epoch: 300, loss: 0.26444533, val loss: 0.3271574\n",
      "(rep layer 2) epoch: 325, loss: 0.27324593, val loss: 0.32928118\n",
      "(rep layer 2) epoch: 350, loss: 0.271299, val loss: 0.31852406\n",
      "(rep layer 2) epoch: 375, loss: 0.2683026, val loss: 0.3326786\n",
      "(rep layer 2) epoch: 400, loss: 0.2659993, val loss: 0.33940914\n",
      "(rep layer 2) epoch: 425, loss: 0.275254, val loss: 0.32278186\n",
      "(rep layer 2) epoch: 450, loss: 0.26256657, val loss: 0.33256337\n",
      "(rep layer 2) epoch: 475, loss: 0.2638777, val loss: 0.3261129\n",
      "(rep layer 2) epoch: 500, loss: 0.27327836, val loss: 0.32569832\n",
      "(rep layer 2) epoch: 525, loss: 0.27507448, val loss: 0.33709404\n",
      "(rep layer 2) epoch: 550, loss: 0.26588774, val loss: 0.35263392\n",
      "(rep layer 2) epoch: 575, loss: 0.2697389, val loss: 0.3426772\n",
      "(rep layer 2) epoch: 600, loss: 0.26437554, val loss: 0.33758575\n",
      "(rep layer 2) epoch: 625, loss: 0.27369225, val loss: 0.34441778\n",
      "(rep layer 2) epoch: 650, loss: 0.2613468, val loss: 0.34232014\n",
      "(rep layer 2) epoch: 675, loss: 0.26658297, val loss: 0.32206482\n",
      "(rep layer 2) epoch: 700, loss: 0.2658499, val loss: 0.33013663\n",
      "(rep layer 2) epoch: 725, loss: 0.26759157, val loss: 0.32120866\n",
      "run 0 (rep layer 2) auc: 0.9179\n",
      "run 0 (rep layer 2) imtafe: 18.4\n",
      "Device 0: NVIDIA A10\n",
      "1.0825902\n",
      "(rep layer 0) epoch: 0, loss: 12.966404, val loss: 12.135119\n",
      "(rep layer 0) epoch: 25, loss: 13.068974, val loss: 9.7209635\n",
      "(rep layer 0) epoch: 50, loss: 1.0825902, val loss: 4.011037\n",
      "(rep layer 0) epoch: 75, loss: 1.537681, val loss: 4.1613154\n",
      "(rep layer 0) epoch: 100, loss: 1.2773862, val loss: 2.660798\n",
      "(rep layer 0) epoch: 125, loss: 1.0718175, val loss: 2.8949494\n",
      "(rep layer 0) epoch: 150, loss: 0.6688996, val loss: 3.0304117\n",
      "(rep layer 0) epoch: 175, loss: 2.5044482, val loss: 15.147906\n",
      "(rep layer 0) epoch: 200, loss: 1.0875872, val loss: 3.596331\n",
      "(rep layer 0) epoch: 225, loss: 1.7214543, val loss: 10.738782\n",
      "(rep layer 0) epoch: 250, loss: 0.9631777, val loss: 5.264454\n",
      "(rep layer 0) epoch: 275, loss: 2.2355936, val loss: 2.4107366\n",
      "(rep layer 0) epoch: 300, loss: 0.71253175, val loss: 2.6366851\n",
      "(rep layer 0) epoch: 325, loss: 0.9322746, val loss: 1.4418663\n",
      "(rep layer 0) epoch: 350, loss: 0.7903913, val loss: 1.4747237\n",
      "(rep layer 0) epoch: 375, loss: 0.62175745, val loss: 2.6099524\n",
      "(rep layer 0) epoch: 400, loss: 1.4078826, val loss: 2.480163\n",
      "(rep layer 0) epoch: 425, loss: 1.2024316, val loss: 1.0533508\n",
      "(rep layer 0) epoch: 450, loss: 1.9788393, val loss: 1.5908175\n",
      "(rep layer 0) epoch: 475, loss: 0.53863025, val loss: 2.6186378\n",
      "(rep layer 0) epoch: 500, loss: 1.0304416, val loss: 1.579063\n",
      "(rep layer 0) epoch: 525, loss: 1.413418, val loss: 0.83990705\n",
      "(rep layer 0) epoch: 550, loss: 1.7741174, val loss: 0.7531603\n",
      "(rep layer 0) epoch: 575, loss: 1.0792829, val loss: 0.685354\n",
      "(rep layer 0) epoch: 600, loss: 0.8135359, val loss: 5.713853\n",
      "(rep layer 0) epoch: 625, loss: 0.5750402, val loss: 10.385355\n",
      "(rep layer 0) epoch: 650, loss: 1.8234168, val loss: 3.5169737\n",
      "(rep layer 0) epoch: 675, loss: 2.1611497, val loss: 1.3016905\n",
      "(rep layer 0) epoch: 700, loss: 0.4909377, val loss: 2.957545\n",
      "(rep layer 0) epoch: 725, loss: 0.86167103, val loss: 3.0016325\n",
      "run 1 (rep layer 0) auc: 0.9322\n",
      "run 1 (rep layer 0) imtafe: 19.0\n",
      "Device 0: NVIDIA A10\n",
      "10.046564\n",
      "re-initialized LCT\n",
      "0.9999418\n",
      "(rep layer 1) epoch: 0, loss: 17.641075, val loss: 8.898345\n",
      "(rep layer 1) epoch: 25, loss: 1.3421121, val loss: 11.212469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rep layer 1) epoch: 50, loss: 0.9999418, val loss: 5.523946\n",
      "(rep layer 1) epoch: 75, loss: 8.401289, val loss: 3.6073213\n",
      "(rep layer 1) epoch: 100, loss: 0.92329884, val loss: 3.9422877\n",
      "(rep layer 1) epoch: 125, loss: 2.1512856, val loss: 1.9486511\n",
      "(rep layer 1) epoch: 150, loss: 1.1059268, val loss: 5.268175\n",
      "(rep layer 1) epoch: 175, loss: 1.003024, val loss: 3.9997218\n",
      "(rep layer 1) epoch: 200, loss: 0.6518978, val loss: 2.9528856\n",
      "(rep layer 1) epoch: 225, loss: 1.2670442, val loss: 3.4426644\n",
      "(rep layer 1) epoch: 250, loss: 2.1813362, val loss: 1.5459422\n",
      "(rep layer 1) epoch: 275, loss: 1.0555383, val loss: 5.886836\n",
      "(rep layer 1) epoch: 300, loss: 1.0326312, val loss: 1.1928298\n",
      "(rep layer 1) epoch: 325, loss: 0.8550622, val loss: 1.6839293\n",
      "(rep layer 1) epoch: 350, loss: 0.80870634, val loss: 3.482765\n",
      "(rep layer 1) epoch: 375, loss: 0.5692836, val loss: 16.092722\n",
      "(rep layer 1) epoch: 400, loss: 0.93458337, val loss: 1.3406739\n",
      "(rep layer 1) epoch: 425, loss: 0.6674718, val loss: 1.0127802\n",
      "(rep layer 1) epoch: 450, loss: 0.98161316, val loss: 2.8860357\n",
      "(rep layer 1) epoch: 475, loss: 0.6931977, val loss: 0.89803725\n",
      "(rep layer 1) epoch: 500, loss: 1.6917319, val loss: 1.1392053\n",
      "(rep layer 1) epoch: 525, loss: 1.4118702, val loss: 1.2385993\n",
      "(rep layer 1) epoch: 550, loss: 1.6756257, val loss: 1.1461028\n",
      "(rep layer 1) epoch: 575, loss: 0.9456076, val loss: 0.7171451\n",
      "(rep layer 1) epoch: 600, loss: 0.64745426, val loss: 0.63745314\n",
      "(rep layer 1) epoch: 625, loss: 1.2027612, val loss: 0.7360146\n",
      "(rep layer 1) epoch: 650, loss: 0.51294595, val loss: 0.73655653\n",
      "(rep layer 1) epoch: 675, loss: 2.8736398, val loss: 5.781805\n",
      "(rep layer 1) epoch: 700, loss: 0.79976296, val loss: 4.590649\n",
      "(rep layer 1) epoch: 725, loss: 0.6572659, val loss: 1.9090455\n",
      "run 1 (rep layer 1) auc: 0.92\n",
      "run 1 (rep layer 1) imtafe: 15.7\n",
      "Device 0: NVIDIA A10\n",
      "0.27544928\n",
      "(rep layer 2) epoch: 0, loss: 0.5464236, val loss: 0.37931657\n",
      "(rep layer 2) epoch: 25, loss: 0.3111091, val loss: 0.35907632\n",
      "(rep layer 2) epoch: 50, loss: 0.27544928, val loss: 0.36199665\n",
      "(rep layer 2) epoch: 75, loss: 0.27374566, val loss: 0.3658716\n",
      "(rep layer 2) epoch: 100, loss: 0.2746858, val loss: 0.36493045\n",
      "(rep layer 2) epoch: 125, loss: 0.2822293, val loss: 0.3559738\n",
      "(rep layer 2) epoch: 150, loss: 0.27732596, val loss: 0.35290858\n",
      "(rep layer 2) epoch: 175, loss: 0.26540384, val loss: 0.35249433\n",
      "(rep layer 2) epoch: 200, loss: 0.27677715, val loss: 0.36079666\n",
      "(rep layer 2) epoch: 225, loss: 0.27364367, val loss: 0.36771208\n",
      "(rep layer 2) epoch: 250, loss: 0.27627903, val loss: 0.36156675\n",
      "(rep layer 2) epoch: 275, loss: 0.26951805, val loss: 0.3477419\n",
      "(rep layer 2) epoch: 300, loss: 0.2645718, val loss: 0.36219227\n",
      "(rep layer 2) epoch: 325, loss: 0.25985113, val loss: 0.3713786\n",
      "(rep layer 2) epoch: 350, loss: 0.2619512, val loss: 0.36159292\n",
      "(rep layer 2) epoch: 375, loss: 0.26613873, val loss: 0.35147715\n",
      "(rep layer 2) epoch: 400, loss: 0.2613543, val loss: 0.3737682\n",
      "(rep layer 2) epoch: 425, loss: 0.266813, val loss: 0.36918476\n",
      "(rep layer 2) epoch: 450, loss: 0.27675608, val loss: 0.35996023\n",
      "(rep layer 2) epoch: 475, loss: 0.26733908, val loss: 0.34557357\n",
      "(rep layer 2) epoch: 500, loss: 0.26119602, val loss: 0.3556731\n",
      "(rep layer 2) epoch: 525, loss: 0.26595867, val loss: 0.38589966\n",
      "(rep layer 2) epoch: 550, loss: 0.2679294, val loss: 0.369055\n",
      "(rep layer 2) epoch: 575, loss: 0.2739893, val loss: 0.38086024\n",
      "(rep layer 2) epoch: 600, loss: 0.26604095, val loss: 0.36369514\n",
      "(rep layer 2) epoch: 625, loss: 0.2629927, val loss: 0.36647743\n",
      "(rep layer 2) epoch: 650, loss: 0.26565638, val loss: 0.367694\n",
      "(rep layer 2) epoch: 675, loss: 0.26039997, val loss: 0.3667459\n",
      "(rep layer 2) epoch: 700, loss: 0.26098087, val loss: 0.36434153\n",
      "(rep layer 2) epoch: 725, loss: 0.25897825, val loss: 0.36564934\n",
      "run 1 (rep layer 2) auc: 0.9159\n",
      "run 1 (rep layer 2) imtafe: 16.4\n",
      "Device 0: NVIDIA A10\n",
      "49.474827\n",
      "re-initialized LCT\n",
      "49.82639\n",
      "re-initialized LCT\n",
      "2.0266702\n",
      "(rep layer 0) epoch: 0, loss: 18.256071, val loss: 11.440506\n",
      "(rep layer 0) epoch: 25, loss: 2.5792682, val loss: 13.418434\n",
      "(rep layer 0) epoch: 50, loss: 2.0266702, val loss: 7.1868143\n",
      "(rep layer 0) epoch: 75, loss: 1.0837007, val loss: 12.182018\n",
      "(rep layer 0) epoch: 100, loss: 2.134567, val loss: 17.224894\n",
      "(rep layer 0) epoch: 125, loss: 1.1739866, val loss: 5.7852736\n",
      "(rep layer 0) epoch: 150, loss: 0.8942832, val loss: 8.6202545\n",
      "(rep layer 0) epoch: 175, loss: 0.8407229, val loss: 4.0932455\n",
      "(rep layer 0) epoch: 200, loss: 1.0123105, val loss: 3.4052942\n",
      "(rep layer 0) epoch: 225, loss: 1.8326765, val loss: 11.638087\n",
      "(rep layer 0) epoch: 250, loss: 0.8804126, val loss: 14.336491\n",
      "(rep layer 0) epoch: 275, loss: 1.3009388, val loss: 8.170371\n",
      "(rep layer 0) epoch: 300, loss: 1.214667, val loss: 3.404847\n",
      "(rep layer 0) epoch: 325, loss: 1.0345984, val loss: 4.7102656\n",
      "(rep layer 0) epoch: 350, loss: 0.8331217, val loss: 3.2480378\n",
      "(rep layer 0) epoch: 375, loss: 0.8908295, val loss: 2.573177\n",
      "(rep layer 0) epoch: 400, loss: 0.816413, val loss: 19.32191\n",
      "(rep layer 0) epoch: 425, loss: 1.133471, val loss: 4.0278473\n",
      "(rep layer 0) epoch: 450, loss: 2.4215863, val loss: 1.9787458\n",
      "(rep layer 0) epoch: 475, loss: 0.9181325, val loss: 8.14715\n",
      "(rep layer 0) epoch: 500, loss: 0.97279334, val loss: 3.661217\n",
      "(rep layer 0) epoch: 525, loss: 0.67414665, val loss: 5.714159\n",
      "(rep layer 0) epoch: 550, loss: 0.7222946, val loss: 2.9290621\n",
      "(rep layer 0) epoch: 575, loss: 1.9152088, val loss: 2.0571373\n",
      "(rep layer 0) epoch: 600, loss: 1.4287815, val loss: 5.0171757\n",
      "(rep layer 0) epoch: 625, loss: 0.7239259, val loss: 2.322295\n",
      "(rep layer 0) epoch: 650, loss: 1.8119054, val loss: 1.3468928\n",
      "(rep layer 0) epoch: 675, loss: 0.53737974, val loss: 1.5558712\n",
      "(rep layer 0) epoch: 700, loss: 0.5607342, val loss: 2.098006\n",
      "(rep layer 0) epoch: 725, loss: 0.76923966, val loss: 1.6035191\n",
      "run 2 (rep layer 0) auc: 0.9278\n",
      "run 2 (rep layer 0) imtafe: 17.0\n",
      "Device 0: NVIDIA A10\n",
      "49.453125\n",
      "re-initialized LCT\n",
      "2.0180535\n",
      "(rep layer 1) epoch: 0, loss: 13.573713, val loss: 9.044631\n",
      "(rep layer 1) epoch: 25, loss: 1.4761871, val loss: 6.8665657\n",
      "(rep layer 1) epoch: 50, loss: 2.0180535, val loss: 5.1940346\n",
      "(rep layer 1) epoch: 75, loss: 0.91715646, val loss: 3.8220007\n",
      "(rep layer 1) epoch: 100, loss: 1.8697408, val loss: 4.726392\n",
      "(rep layer 1) epoch: 125, loss: 0.66725177, val loss: 4.164931\n",
      "(rep layer 1) epoch: 150, loss: 0.62835646, val loss: 4.4183936\n",
      "(rep layer 1) epoch: 175, loss: 0.7661965, val loss: 4.046058\n",
      "(rep layer 1) epoch: 200, loss: 4.2148495, val loss: 2.5594063\n",
      "(rep layer 1) epoch: 225, loss: 1.0325283, val loss: 2.2842083\n",
      "(rep layer 1) epoch: 250, loss: 1.5447663, val loss: 1.8163389\n",
      "(rep layer 1) epoch: 275, loss: 0.5449562, val loss: 1.6515965\n",
      "(rep layer 1) epoch: 300, loss: 0.7537366, val loss: 1.1282152\n",
      "(rep layer 1) epoch: 325, loss: 1.3245466, val loss: 1.5287704\n",
      "(rep layer 1) epoch: 350, loss: 0.83328557, val loss: 3.1700768\n",
      "(rep layer 1) epoch: 375, loss: 0.70476824, val loss: 1.7599484\n",
      "(rep layer 1) epoch: 400, loss: 0.8626663, val loss: 8.484\n",
      "(rep layer 1) epoch: 425, loss: 4.3150873, val loss: 1.5045611\n",
      "(rep layer 1) epoch: 450, loss: 0.46073258, val loss: 1.3559439\n",
      "(rep layer 1) epoch: 475, loss: 0.64554715, val loss: 0.8971026\n",
      "(rep layer 1) epoch: 500, loss: 0.8475848, val loss: 3.7260065\n",
      "(rep layer 1) epoch: 525, loss: 0.86815166, val loss: 2.906256\n",
      "(rep layer 1) epoch: 550, loss: 0.78254807, val loss: 1.2425183\n",
      "(rep layer 1) epoch: 575, loss: 3.3844776, val loss: 2.5668197\n",
      "(rep layer 1) epoch: 600, loss: 0.7900928, val loss: 1.0381609\n",
      "(rep layer 1) epoch: 625, loss: 0.79698676, val loss: 1.1231393\n",
      "(rep layer 1) epoch: 650, loss: 0.6060797, val loss: 7.806736\n",
      "(rep layer 1) epoch: 675, loss: 0.7720086, val loss: 1.6299083\n",
      "(rep layer 1) epoch: 700, loss: 0.79540324, val loss: 1.4387972\n",
      "(rep layer 1) epoch: 725, loss: 0.95222026, val loss: 1.2117276\n",
      "run 2 (rep layer 1) auc: 0.9001\n",
      "run 2 (rep layer 1) imtafe: 11.8\n",
      "Device 0: NVIDIA A10\n",
      "0.28893244\n",
      "(rep layer 2) epoch: 0, loss: 0.5535133, val loss: 0.4443867\n",
      "(rep layer 2) epoch: 25, loss: 0.29392308, val loss: 0.39727762\n",
      "(rep layer 2) epoch: 50, loss: 0.28893244, val loss: 0.4089085\n",
      "(rep layer 2) epoch: 75, loss: 0.27540955, val loss: 0.3990493\n",
      "(rep layer 2) epoch: 100, loss: 0.26901984, val loss: 0.3901488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rep layer 2) epoch: 125, loss: 0.2655799, val loss: 0.41207266\n",
      "(rep layer 2) epoch: 150, loss: 0.2828299, val loss: 0.39262837\n",
      "(rep layer 2) epoch: 175, loss: 0.26476574, val loss: 0.4065502\n",
      "(rep layer 2) epoch: 200, loss: 0.26954925, val loss: 0.41225624\n",
      "(rep layer 2) epoch: 225, loss: 0.26688206, val loss: 0.4406068\n",
      "(rep layer 2) epoch: 250, loss: 0.26234972, val loss: 0.44570258\n",
      "(rep layer 2) epoch: 275, loss: 0.25896657, val loss: 0.3983139\n",
      "(rep layer 2) epoch: 300, loss: 0.27258623, val loss: 0.41636336\n",
      "(rep layer 2) epoch: 325, loss: 0.2601922, val loss: 0.40453732\n",
      "(rep layer 2) epoch: 350, loss: 0.26122904, val loss: 0.40331036\n",
      "(rep layer 2) epoch: 375, loss: 0.26159173, val loss: 0.4393117\n",
      "(rep layer 2) epoch: 400, loss: 0.26568615, val loss: 0.41451687\n",
      "(rep layer 2) epoch: 425, loss: 0.25714728, val loss: 0.3981221\n",
      "(rep layer 2) epoch: 450, loss: 0.2519011, val loss: 0.40180925\n",
      "(rep layer 2) epoch: 475, loss: 0.25791037, val loss: 0.42515492\n",
      "(rep layer 2) epoch: 500, loss: 0.25283226, val loss: 0.42179134\n",
      "(rep layer 2) epoch: 525, loss: 0.25936425, val loss: 0.3886739\n",
      "(rep layer 2) epoch: 550, loss: 0.2682576, val loss: 0.42045248\n",
      "(rep layer 2) epoch: 575, loss: 0.2667283, val loss: 0.3963464\n",
      "(rep layer 2) epoch: 600, loss: 0.26275194, val loss: 0.42313954\n",
      "(rep layer 2) epoch: 625, loss: 0.26453316, val loss: 0.41379568\n",
      "(rep layer 2) epoch: 650, loss: 0.26735955, val loss: 0.39825794\n",
      "(rep layer 2) epoch: 675, loss: 0.25488406, val loss: 0.4225684\n",
      "(rep layer 2) epoch: 700, loss: 0.25324294, val loss: 0.40925375\n",
      "(rep layer 2) epoch: 725, loss: 0.2703888, val loss: 0.40843344\n",
      "run 2 (rep layer 2) auc: 0.9098\n",
      "run 2 (rep layer 2) imtafe: 15.4\n"
     ]
    }
   ],
   "source": [
    "# final LCT for each rep layer\n",
    "auc_lst, imt_lst = [], []\n",
    "for run in range(3):\n",
    "    for i in range(vl_reps_1.shape[1]):\n",
    "#         if i == 1:\n",
    "        out_dat_f, out_lbs_f, losses_f, val_losses_f = linear_classifier_test( linear_input_size, linear_batch_size, linear_n_epochs, \"adam\", linear_learning_rate, vl_reps_1[:,i,:], np.expand_dims(vl_lab_1, axis=1), vl_reps_2[:,i,:], np.expand_dims(vl_lab_2, axis=1) )\n",
    "        auc, imtafe = get_perf_stats( out_lbs_f, out_dat_f )\n",
    "        ep=0\n",
    "        step_size = 25\n",
    "        for (lss, val_lss) in zip(losses_f[::step_size], val_losses_f):\n",
    "            print( f\"(rep layer {i}) epoch: \" + str( ep ) + \", loss: \" + str( lss ) + \", val loss: \" + str( val_lss ), flush=True)\n",
    "            ep+=step_size\n",
    "        print( f\"run {run} (rep layer {i}) auc: \"+str( round(auc, 4) ), flush=True )\n",
    "        print( f\"run {run} (rep layer {i}) imtafe: \"+str( round(imtafe, 1) ), flush=True)\n",
    "        auc_lst.append(round(auc, 4))\n",
    "        imt_lst.append(round(imtafe, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088459e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:19:05.055806Z",
     "start_time": "2024-02-07T00:19:05.050598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-152.35652  ,  -31.516916 ,  -30.420609 , ...,  -35.251858 ,\n",
       "           2.250964 ,   -4.41827  ],\n",
       "       [-103.33725  ,  -59.772614 ,  -26.388754 , ...,  -87.4075   ,\n",
       "        -142.72273  , -135.32193  ],\n",
       "       [ -70.02197  ,  -28.411247 ,  -15.907927 , ...,  -47.56984  ,\n",
       "        -115.01345  , -115.87604  ],\n",
       "       ...,\n",
       "       [-106.91914  ,  -37.978436 ,  -23.446894 , ..., -110.92433  ,\n",
       "        -109.37837  , -101.36799  ],\n",
       "       [ -37.637566 ,  -66.314896 ,  -17.828943 , ..., -167.56693  ,\n",
       "         -94.2389   ,  -95.51067  ],\n",
       "       [ -80.782295 ,  -64.91017  ,  -51.30025  , ..., -157.12285  ,\n",
       "         -66.60137  ,   -7.9691277]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_reps_1[:10, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b094efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:20:05.406075Z",
     "start_time": "2024-02-07T00:20:05.403698Z"
    }
   },
   "outputs": [],
   "source": [
    "auc_lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "307598f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:20:05.845837Z",
     "start_time": "2024-02-07T00:20:05.844071Z"
    }
   },
   "outputs": [],
   "source": [
    "imt_lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c420f2e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:20:16.147238Z",
     "start_time": "2024-02-07T00:20:16.143993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9001, 0.9098, 0.9159, 0.9179, 0.92, 0.9278, 0.9322, 0.9323, 0.9329]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb47060f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:20:21.072483Z",
     "start_time": "2024-02-07T00:20:21.069220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.8, 15.4, 15.7, 16.4, 17.0, 18.4, 19.0, 19.1, 21.9]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imt_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e26edca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T04:19:22.255602Z",
     "start_time": "2024-02-07T04:19:22.249928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- hyper-parameters ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- hyper-parameters ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03476dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
